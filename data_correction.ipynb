
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6d7e06a",
   "metadata": {},
   "source": [
    "correcting data to improve models :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9cdfa8",
   "metadata": {},
   "source": [
    "avenue : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e32f8eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30652/30652 [04:41<00:00, 108.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Done. All frames copied and renamed into 'combined_data/avenue/combined_frames'\n",
      "ðŸ“„ Updated CSV saved as 'combined_data/avenue/combined_data.csv' with new paths.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ==== USER: MODIFY THESE ====\n",
    "csv_file1 = 'data/avenue/test_labels.csv'\n",
    "csv_file2 = 'data/avenue/train_labels.csv'\n",
    "combined_folder = 'combined_data/avenue/combined_frames'\n",
    "output_csv = 'combined_data/avenue/combined_data.csv'\n",
    "# ============================\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "os.makedirs(combined_folder, exist_ok=True)\n",
    "\n",
    "# Load CSVs\n",
    "df1 = pd.read_csv(csv_file1)\n",
    "df2 = pd.read_csv(csv_file2)\n",
    "\n",
    "# Combine them\n",
    "combined_df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# Column names (change if yours are different)\n",
    "path_col = 'path'\n",
    "label_col = 'label'\n",
    "\n",
    "# New rows to save in the output CSV\n",
    "new_rows = []\n",
    "\n",
    "# Iterate with progress bar\n",
    "for i, row in tqdm(combined_df.iterrows(), total=len(combined_df), desc=\"Processing frames\"):\n",
    "    original_path = row[path_col]\n",
    "    label = row[label_col]\n",
    "\n",
    "    # Ensure the original path exists\n",
    "    if not os.path.exists(original_path):\n",
    "        tqdm.write(f\"[Warning] File not found: {original_path}\")\n",
    "        continue\n",
    "\n",
    "    # Generate a unique filename to avoid collisions\n",
    "    file_ext = os.path.splitext(original_path)[1]  # keep original extension\n",
    "    new_filename = f\"frame_{i:05d}{file_ext}\"      # e.g., frame_00001.jpg\n",
    "    new_path = os.path.join(combined_folder, new_filename)\n",
    "\n",
    "    # Copy the file\n",
    "    shutil.copy2(original_path, new_path)\n",
    "\n",
    "    # Save new path and label\n",
    "    new_rows.append([new_path, label])\n",
    "\n",
    "# Save the new CSV\n",
    "output_df = pd.DataFrame(new_rows, columns=[path_col, label_col])\n",
    "output_df.to_csv(output_csv, index=False)\n",
    "\n",
    "print(f\"\\nâœ… Done. All frames copied and renamed into '{combined_folder}'\")\n",
    "print(f\"ðŸ“„ Updated CSV saved as '{output_csv}' with new paths.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c69c049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¢ Label Counts:\n",
      "label\n",
      "0    26051\n",
      "1     4601\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total 0s: 26051\n",
      "Total 1s: 4601\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to your combined CSV\n",
    "csv_path = 'combined_data/avenue/combined_data.csv'\n",
    "\n",
    "# Load the CSV\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Count values in 'label' column\n",
    "label_counts = df['label'].value_counts()\n",
    "\n",
    "# Print results\n",
    "print(\"ðŸ”¢ Label Counts:\")\n",
    "print(label_counts)\n",
    "\n",
    "# Optional: show count for each label explicitly\n",
    "print(f\"\\nTotal 0s: {label_counts.get(0, 0)}\")\n",
    "print(f\"Total 1s: {label_counts.get(1, 0)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e91e82",
   "metadata": {},
   "source": [
    "Read combined_data.csv with paths and labels.\n",
    "\n",
    "1- Undersample label 0:\n",
    "\n",
    "Keep all label 1 frames.\n",
    "\n",
    "From label 0, take 1 frame, skip the next 4, repeat.\n",
    "\n",
    "2- Split the resulting data into:\n",
    "\n",
    "70% training â†’ move to folder avenue_train/\n",
    "\n",
    "30% testing â†’ move to folder test_avenue/\n",
    "\n",
    "3- Save new CSV files for both sets: train.csv, test.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa9a25c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying to combined_data/avenue/avenue_train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6868/6868 [04:41<00:00, 24.44it/s]\n",
      "Copying to combined_data/avenue/test_avenue: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2944/2944 [02:11<00:00, 22.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Finished. Train â†’ 6868 samples. Test â†’ 2944 samples.\n",
      "ðŸ“‚ Train folder: combined_data/avenue/avenue_train | CSV: combined_data/avenue/train.csv\n",
      "ðŸ“‚ Test folder: combined_data/avenue/test_avenue | CSV: combined_data/avenue/test.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === CONFIG ===\n",
    "input_csv = 'combined_data/avenue/combined_data.csv'\n",
    "train_dir = 'combined_data/avenue/avenue_train'\n",
    "test_dir = 'combined_data/avenue/test_avenue'\n",
    "train_csv = 'combined_data/avenue/train.csv'\n",
    "test_csv = 'combined_data/avenue/test.csv'\n",
    "undersample_ratio = 5  # keep 1 every 5 label 0 frames\n",
    "# ==============\n",
    "\n",
    "# Step 1: Load CSV\n",
    "df = pd.read_csv(input_csv)\n",
    "\n",
    "# Step 2: Undersample label 0\n",
    "label_1_df = df[df['label'] == 1]\n",
    "label_0_df = df[df['label'] == 0].reset_index(drop=True)\n",
    "label_0_sampled = label_0_df.iloc[::undersample_ratio]\n",
    "\n",
    "# Step 3: Combine balanced data\n",
    "balanced_df = pd.concat([label_1_df, label_0_sampled], ignore_index=True).sample(frac=1, random_state=42)\n",
    "\n",
    "# Step 4: Train-test split\n",
    "train_df, test_df = train_test_split(balanced_df, test_size=0.3, random_state=42, stratify=balanced_df['label'])\n",
    "\n",
    "# Step 5: Create output folders\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "def move_files(df, target_folder):\n",
    "    updated_rows = []\n",
    "    for i, row in tqdm(df.iterrows(), total=len(df), desc=f\"Copying to {target_folder}\"):\n",
    "        old_path = row['path']\n",
    "        label = row['label']\n",
    "        filename = os.path.basename(old_path)\n",
    "        new_path = os.path.join(target_folder, filename)\n",
    "\n",
    "        if os.path.exists(old_path):\n",
    "            shutil.copy2(old_path, new_path)\n",
    "            updated_rows.append([new_path, label])\n",
    "        else:\n",
    "            tqdm.write(f\"[Warning] Missing file: {old_path}\")\n",
    "\n",
    "    return pd.DataFrame(updated_rows, columns=['path', 'label'])\n",
    "\n",
    "# Step 6: Move files and save new CSVs\n",
    "train_final_df = move_files(train_df, train_dir)\n",
    "test_final_df = move_files(test_df, test_dir)\n",
    "\n",
    "train_final_df.to_csv(train_csv, index=False)\n",
    "test_final_df.to_csv(test_csv, index=False)\n",
    "\n",
    "print(f\"\\nâœ… Finished. Train â†’ {len(train_final_df)} samples. Test â†’ {len(test_final_df)} samples.\")\n",
    "print(f\"ðŸ“‚ Train folder: {train_dir} | CSV: {train_csv}\")\n",
    "print(f\"ðŸ“‚ Test folder: {test_dir} | CSV: {test_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d128d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¢ train label Counts:\n",
      "label\n",
      "0    3647\n",
      "1    3221\n",
      "Name: count, dtype: int64\n",
      "ðŸ”¢ test label Counts:\n",
      "label\n",
      "0    1564\n",
      "1    1380\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to your combined CSV\n",
    "csv_path1 = 'combined_data/avenue/train.csv'\n",
    "csv_path2 = 'combined_data/avenue/test.csv'\n",
    "\n",
    "# Load the CSV\n",
    "df1 = pd.read_csv(csv_path1)\n",
    "df2 = pd.read_csv(csv_path2)\n",
    "\n",
    "# Count values in 'label' column\n",
    "label_counts1 = df1['label'].value_counts()\n",
    "label_counts2 = df2['label'].value_counts()\n",
    "\n",
    "# Print results\n",
    "print(\"ðŸ”¢ train label Counts:\")\n",
    "print(label_counts1)\n",
    "print(\"ðŸ”¢ test label Counts:\")\n",
    "print(label_counts2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015a951f",
   "metadata": {},
   "source": [
    "violent flows :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1771c146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¢ train label Counts:\n",
      "label\n",
      "1    12530\n",
      "0     9544\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to your combined CSV\n",
    "csv_path1 = 'data/Violent-Flows/violentflows_labels.csv'\n",
    "# Load the CSV\n",
    "df1 = pd.read_csv(csv_path1)\n",
    "\n",
    "# Count values in 'label' column\n",
    "label_counts1 = df1['label'].value_counts()\n",
    "\n",
    "# Print results\n",
    "print(\"ðŸ”¢ train label Counts:\")\n",
    "print(label_counts1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5befd7b9",
   "metadata": {},
   "source": [
    "so the violent_flows dataset doesn't need any fixing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35321e4",
   "metadata": {},
   "source": [
    "fixing ped1 and ped2 datasets :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2daaf10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¢ train label Counts:\n",
      "label\n",
      "0    6800\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "csv_path1 = 'data/UCSD_Anomaly_Dataset.v1p2/labels_ped1_train.csv'\n",
    "df1 = pd.read_csv(csv_path1)\n",
    "label_counts1 = df1['label'].value_counts()\n",
    "print(\"ðŸ”¢ train label Counts:\")\n",
    "print(label_counts1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9568cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¢ test label Counts:\n",
      "label\n",
      "0    5964\n",
      "1    1235\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv('data/UCSD_Anomaly_Dataset.v1p2/labels_ped1_test.csv')\n",
    "print(\"ðŸ”¢ test label Counts:\")\n",
    "print(df1[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "853af02c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¢ test label Counts:\n",
      "label\n",
      "1    1276\n",
      "0     734\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv('data/UCSD_Anomaly_Dataset.v1p2/labels_ped2_test.csv')\n",
    "print(\"ðŸ”¢ test label Counts:\")\n",
    "print(df1[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60de621a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¢ test label Counts:\n",
      "label\n",
      "0    6800\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv('data/UCSD_Anomaly_Dataset.v1p2/labels_ped1_train.csv')\n",
    "print(\"ðŸ”¢ test label Counts:\")\n",
    "print(df1[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65408fd8",
   "metadata": {},
   "source": [
    "so we are gonna take the excessive abnormal frames from the violents flows dataset and we are gonna divide them between the ped1 and ped2 since they have very small amount in the abnormal frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "750947ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“ Processing PED1...\n",
      "âž¡ï¸  Processing TRAIN set: data/UCSD_Anomaly_Dataset.v1p2/labels_ped1_train.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âž¡ï¸  Processing TEST set: data/UCSD_Anomaly_Dataset.v1p2/labels_ped1_test.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Finished PED1: 13999 total frames.\n",
      "ðŸ“„ CSV saved: combined_data\\UCSD_ped1\\ped1_merged.csv\n",
      "\n",
      "ðŸ“ Processing PED2...\n",
      "âž¡ï¸  Processing TRAIN set: data/UCSD_Anomaly_Dataset.v1p2/labels_ped2_train.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âž¡ï¸  Processing TEST set: data/UCSD_Anomaly_Dataset.v1p2/labels_ped2_test.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Finished PED2: 4560 total frames.\n",
      "ðŸ“„ CSV saved: combined_data\\UCSD_ped2\\ped2_merged.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ===== Paths to CSV label files =====\n",
    "csv_files = {\n",
    "    \"ped1\": [\n",
    "        (\"train\", \"data/UCSD_Anomaly_Dataset.v1p2/labels_ped1_train.csv\"),\n",
    "        (\"test\", \"data/UCSD_Anomaly_Dataset.v1p2/labels_ped1_test.csv\")\n",
    "    ],\n",
    "    \"ped2\": [\n",
    "        (\"train\", \"data/UCSD_Anomaly_Dataset.v1p2/labels_ped2_train.csv\"),\n",
    "        (\"test\", \"data/UCSD_Anomaly_Dataset.v1p2/labels_ped2_test.csv\")\n",
    "    ]\n",
    "}\n",
    "\n",
    "# ===== Output base folder =====\n",
    "output_base = \"combined_data\"\n",
    "\n",
    "# ===== Process each group =====\n",
    "for group, file_list in csv_files.items():\n",
    "    print(f\"\\nðŸ“ Processing {group.upper()}...\")\n",
    "\n",
    "    # Output merged folder\n",
    "    merged_folder = os.path.join(output_base, f\"UCSD_{group}\", f\"{group}_merged\")\n",
    "    os.makedirs(merged_folder, exist_ok=True)\n",
    "\n",
    "    # List to collect updated [new_path, label]\n",
    "    new_entries = []\n",
    "    frame_counter = 0\n",
    "\n",
    "    # Read and process each CSV\n",
    "    for split_name, csv_file in file_list:\n",
    "        print(f\"âž¡ï¸  Processing {split_name.upper()} set: {csv_file}\")\n",
    "        df = pd.read_csv(csv_file)\n",
    "\n",
    "        for _, row in tqdm(df.iterrows(), total=len(df), desc=f\"Copying {group} {split_name}\", leave=False):\n",
    "            original_path = row[\"path\"]\n",
    "            label = row[\"label\"]\n",
    "\n",
    "            if not os.path.exists(original_path):\n",
    "                tqdm.write(f\"âš ï¸ Missing file: {original_path}\")\n",
    "                continue\n",
    "\n",
    "            # Generate a unique filename\n",
    "            ext = os.path.splitext(original_path)[1]\n",
    "            new_filename = f\"{group}_frame_{frame_counter:05d}{ext}\"\n",
    "            new_path = os.path.join(merged_folder, new_filename)\n",
    "\n",
    "            # Copy file\n",
    "            shutil.copy2(original_path, new_path)\n",
    "\n",
    "            # Save updated row\n",
    "            new_entries.append([new_path, label])\n",
    "            frame_counter += 1\n",
    "\n",
    "    # Save merged CSV\n",
    "    df_output = pd.DataFrame(new_entries, columns=[\"path\", \"label\"])\n",
    "    output_csv_path = os.path.join(output_base, f\"UCSD_{group}\", f\"{group}_merged.csv\")\n",
    "    df_output.to_csv(output_csv_path, index=False)\n",
    "\n",
    "    print(f\"âœ… Finished {group.upper()}: {frame_counter} total frames.\")\n",
    "    print(f\"ðŸ“„ CSV saved: {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a65b0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¢ped1 label Counts:\n",
      "label\n",
      "0    12764\n",
      "1     1235\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv('combined_data/UCSD_ped1/ped1_merged.csv')\n",
    "print(\"ðŸ”¢ped1 label Counts:\")\n",
    "print(df1[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f31c5a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¢ped2 label Counts:\n",
      "label\n",
      "0    3284\n",
      "1    2769\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv('combined_data/UCSD_ped2/ped2_merged.csv')\n",
    "print(\"ðŸ”¢ped2 label Counts:\")\n",
    "print(df1[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c75409",
   "metadata": {},
   "source": [
    "taking frames from violent flows dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e465b6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying to vf_ped1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1493/1493 [00:57<00:00, 26.14it/s] \n",
      "Copying to vf_ped2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1493/1493 [01:42<00:00, 14.55it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Appended 1493 frames to ped1 and 1493 to ped2.\n",
      "ðŸ§¹ Removed 2986 frames from Violent-Flows and updated the CSV.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "violent_csv_path = \"data/Violent-Flows/violentflows_labels.csv\"\n",
    "ped1_folder = \"combined_data/UCSD_ped1/ped1_merged\"\n",
    "ped2_folder = \"combined_data/UCSD_ped2/ped2_merged\"\n",
    "ped1_csv = \"combined_data/UCSD_ped1/ped1_merged.csv\"\n",
    "ped2_csv = \"combined_data/UCSD_ped2/ped2_merged.csv\"\n",
    "\n",
    "extra_frame_count = 2986\n",
    "half = extra_frame_count // 2\n",
    "# =====================\n",
    "\n",
    "# Step 1: Load Violent-Flows CSV\n",
    "vf_df = pd.read_csv(violent_csv_path)\n",
    "\n",
    "# Step 2: Get first 2,986 label==1 frames\n",
    "violent_frames = vf_df[vf_df[\"label\"] == 1].head(extra_frame_count).reset_index(drop=True)\n",
    "\n",
    "vf_ped1 = violent_frames.iloc[:half]\n",
    "vf_ped2 = violent_frames.iloc[half:]\n",
    "\n",
    "# Helper function to copy frames and create new entries\n",
    "def copy_and_append(vf_rows, target_folder, base_filename_prefix, start_index):\n",
    "    new_entries = []\n",
    "    for i, row in tqdm(vf_rows.iterrows(), total=len(vf_rows), desc=f\"Copying to {base_filename_prefix}\"):\n",
    "        src_path = row['path']\n",
    "        label = row['label']\n",
    "        ext = os.path.splitext(src_path)[1]\n",
    "        new_filename = f\"{base_filename_prefix}_frame_{start_index + i:05d}{ext}\"\n",
    "        dest_path = os.path.join(target_folder, new_filename)\n",
    "\n",
    "        if os.path.exists(src_path):\n",
    "            shutil.copy2(src_path, dest_path)\n",
    "            new_entries.append([dest_path, label])\n",
    "            os.remove(src_path)  # Delete after copying\n",
    "        else:\n",
    "            tqdm.write(f\"âš ï¸ File not found: {src_path}\")\n",
    "\n",
    "    return new_entries\n",
    "\n",
    "# Step 3: Load original UCSD CSVs\n",
    "df_ped1 = pd.read_csv(ped1_csv)\n",
    "df_ped2 = pd.read_csv(ped2_csv)\n",
    "\n",
    "# Step 4: Copy and append to each\n",
    "ped1_new = copy_and_append(vf_ped1, ped1_folder, \"vf_ped1\", len(df_ped1))\n",
    "ped2_new = copy_and_append(vf_ped2, ped2_folder, \"vf_ped2\", len(df_ped2))\n",
    "\n",
    "# Step 5: Update UCSD CSVs\n",
    "df_ped1 = pd.concat([df_ped1, pd.DataFrame(ped1_new, columns=[\"path\", \"label\"])], ignore_index=True)\n",
    "df_ped2 = pd.concat([df_ped2, pd.DataFrame(ped2_new, columns=[\"path\", \"label\"])], ignore_index=True)\n",
    "\n",
    "df_ped1.to_csv(ped1_csv, index=False)\n",
    "df_ped2.to_csv(ped2_csv, index=False)\n",
    "\n",
    "print(f\"âœ… Appended {len(ped1_new)} frames to ped1 and {len(ped2_new)} to ped2.\")\n",
    "\n",
    "# Step 6: Remove used frames from Violent-Flows CSV\n",
    "vf_remaining = vf_df.drop(vf_df[vf_df[\"path\"].isin(violent_frames[\"path\"])].index)\n",
    "vf_remaining.to_csv(violent_csv_path, index=False)\n",
    "\n",
    "print(f\"ðŸ§¹ Removed {len(violent_frames)} frames from Violent-Flows and updated the CSV.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46839c98",
   "metadata": {},
   "source": [
    "testing if the changes occurred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0af0f9",
   "metadata": {},
   "source": [
    "violent flows : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca763dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¢ train label Counts:\n",
      "label\n",
      "0    9544\n",
      "1    9544\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "csv_path1 = 'data/Violent-Flows/violentflows_labels.csv'\n",
    "df1 = pd.read_csv(csv_path1)\n",
    "label_counts1 = df1['label'].value_counts()\n",
    "print(\"ðŸ”¢ train label Counts:\")\n",
    "print(label_counts1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0192dcdf",
   "metadata": {},
   "source": [
    "ped1 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ed66ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¢ped1 label Counts:\n",
      "label\n",
      "0    12764\n",
      "1     2728\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv('combined_data/UCSD_ped1/ped1_merged.csv')\n",
    "print(\"ðŸ”¢ped1 label Counts:\")\n",
    "print(df1[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbcf515",
   "metadata": {},
   "source": [
    "ped2 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b502820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¢ped2 label Counts:\n",
      "label\n",
      "0    3284\n",
      "1    2769\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv('combined_data/UCSD_ped2/ped2_merged.csv')\n",
    "print(\"ðŸ”¢ped2 label Counts:\")\n",
    "print(df1[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ef092c",
   "metadata": {},
   "source": [
    "now we are going to remove the excesive normal frames from ped1 and ped2 to balance the data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f231d73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”§ Processing ped1...\n",
      "Label 0 count: 12764, Label 1 count: 2728\n",
      "ðŸ§¹ Deleting 7308 excessive frames in ped1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting frames from ped1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7308/7308 [00:49<00:00, 148.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Finished ped1: Now {0: 5456, 1: 2728}\n",
      "\n",
      "ðŸ”§ Processing ped2...\n",
      "Label 0 count: 3284, Label 1 count: 2769\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "slice step cannot be zero",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 44\u001b[39m\n\u001b[32m     41\u001b[39m ped2_csv = \u001b[33m\"\u001b[39m\u001b[33mcombined_data/UCSD_ped2/ped2_merged.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     43\u001b[39m reduce_excessive_zeros(ped1_csv, frames_to_keep=\u001b[32m2728\u001b[39m * \u001b[32m2\u001b[39m, label0_total=\u001b[32m12764\u001b[39m, set_name=\u001b[33m\"\u001b[39m\u001b[33mped1\u001b[39m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# keep 2x anomaly\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m \u001b[43mreduce_excessive_zeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43mped2_csv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_to_keep\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2769\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel0_total\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3284\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mset_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mped2\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# same logic\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mreduce_excessive_zeros\u001b[39m\u001b[34m(csv_path, frames_to_keep, label0_total, set_name)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Select frames to keep from label 0 with step to avoid clusters\u001b[39;00m\n\u001b[32m     16\u001b[39m step = \u001b[38;5;28mint\u001b[39m(label0_total / frames_to_keep)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m df_0_kept = \u001b[43mdf_0\u001b[49m\u001b[43m.\u001b[49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m]\u001b[49m.reset_index(drop=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Truncate to exact target\u001b[39;00m\n\u001b[32m     20\u001b[39m df_0_kept = df_0_kept.iloc[:frames_to_keep]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\core\\indexing.py:1191\u001b[39m, in \u001b[36m_LocationIndexer.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1189\u001b[39m maybe_callable = com.apply_if_callable(key, \u001b[38;5;28mself\u001b[39m.obj)\n\u001b[32m   1190\u001b[39m maybe_callable = \u001b[38;5;28mself\u001b[39m._check_deprecated_callable_usage(key, maybe_callable)\n\u001b[32m-> \u001b[39m\u001b[32m1191\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\core\\indexing.py:1729\u001b[39m, in \u001b[36m_iLocIndexer._getitem_axis\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1723\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\n\u001b[32m   1724\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mDataFrame indexer is not allowed for .iloc\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1725\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mConsider using .loc for automatic alignment.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1726\u001b[39m     )\n\u001b[32m   1728\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mslice\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1729\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_slice_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1731\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   1732\u001b[39m     key = \u001b[38;5;28mlist\u001b[39m(key)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\core\\indexing.py:1765\u001b[39m, in \u001b[36m_iLocIndexer._get_slice_axis\u001b[39m\u001b[34m(self, slice_obj, axis)\u001b[39m\n\u001b[32m   1763\u001b[39m labels = obj._get_axis(axis)\n\u001b[32m   1764\u001b[39m labels._validate_positional_slice(slice_obj)\n\u001b[32m-> \u001b[39m\u001b[32m1765\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_slice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mslice_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\core\\generic.py:4388\u001b[39m, in \u001b[36mNDFrame._slice\u001b[39m\u001b[34m(self, slobj, axis)\u001b[39m\n\u001b[32m   4386\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(slobj, \u001b[38;5;28mslice\u001b[39m), \u001b[38;5;28mtype\u001b[39m(slobj)\n\u001b[32m   4387\u001b[39m axis = \u001b[38;5;28mself\u001b[39m._get_block_manager_axis(axis)\n\u001b[32m-> \u001b[39m\u001b[32m4388\u001b[39m new_mgr = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_slice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mslobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4389\u001b[39m result = \u001b[38;5;28mself\u001b[39m._constructor_from_mgr(new_mgr, axes=new_mgr.axes)\n\u001b[32m   4390\u001b[39m result = result.__finalize__(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/internals.pyx:871\u001b[39m, in \u001b[36mpandas._libs.internals.BlockManager.get_slice\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/internals.pyx:852\u001b[39m, in \u001b[36mpandas._libs.internals.BlockManager._slice_mgr_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/internals.pyx:710\u001b[39m, in \u001b[36mpandas._libs.internals.Block.slice_block_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: slice step cannot be zero"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def reduce_excessive_zeros(csv_path, frames_to_keep, label0_total, set_name):\n",
    "    print(f\"\\nðŸ”§ Processing {set_name}...\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Split 0s and 1s\n",
    "    df_0 = df[df[\"label\"] == 0].reset_index(drop=True)\n",
    "    df_1 = df[df[\"label\"] == 1].reset_index(drop=True)\n",
    "\n",
    "    print(f\"Label 0 count: {len(df_0)}, Label 1 count: {len(df_1)}\")\n",
    "\n",
    "    # Select frames to keep from label 0 with step to avoid clusters\n",
    "    step = int(label0_total / frames_to_keep)\n",
    "    df_0_kept = df_0.iloc[::step].reset_index(drop=True)\n",
    "\n",
    "    # Truncate to exact target\n",
    "    df_0_kept = df_0_kept.iloc[:frames_to_keep]\n",
    "\n",
    "    # Frames to delete\n",
    "    to_delete = df_0[~df_0[\"path\"].isin(df_0_kept[\"path\"])]\n",
    "\n",
    "    # Delete excessive 0-label frames\n",
    "    print(f\"ðŸ§¹ Deleting {len(to_delete)} excessive frames in {set_name}...\")\n",
    "    for _, row in tqdm(to_delete.iterrows(), total=len(to_delete), desc=f\"Deleting frames from {set_name}\"):\n",
    "        try:\n",
    "            os.remove(row[\"path\"])\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n",
    "    # Final merged dataframe\n",
    "    final_df = pd.concat([df_0_kept, df_1], ignore_index=True).sample(frac=1).reset_index(drop=True)\n",
    "    final_df.to_csv(csv_path, index=False)\n",
    "\n",
    "    print(f\"âœ… Finished {set_name}: Now {final_df['label'].value_counts().to_dict()}\")\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "ped1_csv = \"combined_data/UCSD_ped1/ped1_merged.csv\"\n",
    "ped2_csv = \"combined_data/UCSD_ped2/ped2_merged.csv\"\n",
    "\n",
    "reduce_excessive_zeros(ped1_csv, frames_to_keep=2728 * 2, label0_total=12764, set_name=\"ped1\")  # keep 2x anomaly\n",
    "reduce_excessive_zeros(ped2_csv, frames_to_keep=2769 , label0_total=3284, set_name=\"ped2\")  # same logic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f55d3e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¢ped1 label Counts:\n",
      "label\n",
      "0    5456\n",
      "1    2728\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv('combined_data/UCSD_ped1/ped1_merged.csv')\n",
    "print(\"ðŸ”¢ped1 label Counts:\")\n",
    "print(df1[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "820cd2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¢ped2 label Counts:\n",
      "label\n",
      "0    3284\n",
      "1    2769\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv('combined_data/UCSD_ped2/ped2_merged.csv')\n",
    "print(\"ðŸ”¢ped2 label Counts:\")\n",
    "print(df1[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ce04a0",
   "metadata": {},
   "source": [
    "creating train and test folders : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d10f1004",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying to combined_data/UCSD_ped1/ped1_train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5728/5728 [00:17<00:00, 324.72it/s]\n",
      "Copying to combined_data/UCSD_ped1/ped1_test: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2456/2456 [00:15<00:00, 154.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Finished. Train â†’ 5728 samples. Test â†’ 2456 samples.\n",
      "ðŸ“‚ Train folder: combined_data/UCSD_ped1/ped1_train | CSV: combined_data/UCSD_ped1/ped1_train.csv\n",
      "ðŸ“‚ Test folder: combined_data/UCSD_ped1/ped1_test | CSV: combined_data/UCSD_ped1/ped1_test.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === CONFIG ===\n",
    "input_csv = 'combined_data/UCSD_ped1/ped1_merged.csv'\n",
    "train_dir = 'combined_data/UCSD_ped1/ped1_train'\n",
    "test_dir = 'combined_data/UCSD_ped1/ped1_test'\n",
    "train_csv = 'combined_data/UCSD_ped1/ped1_train.csv'\n",
    "test_csv = 'combined_data/UCSD_ped1/ped1_test.csv'\n",
    "# ==============\n",
    "\n",
    "# Step 1: Load CSV\n",
    "df = pd.read_csv(input_csv)\n",
    "\n",
    "# Step 2: Train-test split (no undersampling)\n",
    "train_df, test_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=df['label']\n",
    ")\n",
    "\n",
    "# Step 3: Create folders\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "def move_files(df, target_folder):\n",
    "    updated_rows = []\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=f\"Copying to {target_folder}\"):\n",
    "        old_path = row['path']\n",
    "        label = row['label']\n",
    "        filename = os.path.basename(old_path)\n",
    "        new_path = os.path.join(target_folder, filename)\n",
    "\n",
    "        if os.path.exists(old_path):\n",
    "            shutil.copy2(old_path, new_path)\n",
    "            updated_rows.append([new_path, label])\n",
    "        else:\n",
    "            tqdm.write(f\"[Warning] Missing file: {old_path}\")\n",
    "\n",
    "    return pd.DataFrame(updated_rows, columns=['path', 'label'])\n",
    "\n",
    "# Step 4: Copy files and save updated CSVs\n",
    "train_final_df = move_files(train_df, train_dir)\n",
    "test_final_df = move_files(test_df, test_dir)\n",
    "\n",
    "train_final_df.to_csv(train_csv, index=False)\n",
    "test_final_df.to_csv(test_csv, index=False)\n",
    "\n",
    "print(f\"\\nâœ… Finished. Train â†’ {len(train_final_df)} samples. Test â†’ {len(test_final_df)} samples.\")\n",
    "print(f\"ðŸ“‚ Train folder: {train_dir} | CSV: {train_csv}\")\n",
    "print(f\"ðŸ“‚ Test folder: {test_dir} | CSV: {test_csv}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
